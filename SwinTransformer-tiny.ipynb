{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKXxtjvicwPNapDNzhVtzF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LJd4Twtyk7PM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MaROjIy_k5Ws"},"outputs":[],"source":["import os\n","import tarfile\n","\n","data_dir = '/content/drive/MyDrive/심층신경망개론/'\n","images_tar_path = os.path.join(data_dir, '102flowers.tgz')\n","extracted_dir = os.path.join(data_dir, 'jpg')\n","\n","\n","if not os.path.exists(extracted_dir):\n","    print(\"이미지 데이터 추출 중...\")\n","    with tarfile.open(images_tar_path, 'r:gz') as tar:\n","        tar.extractall(path=data_dir)\n","    print(\"이미지 데이터 추출 완료.\")\n","else:\n","    print(\"이미지 데이터가 이미 추출되어 있습니다.\")\n","\n","\n","import scipy.io\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from shutil import copyfile\n","\n","labels_mat_path = os.path.join(data_dir, 'imagelabels.mat')\n","labels_mat = scipy.io.loadmat(labels_mat_path)\n","labels = labels_mat['labels'][0]\n","\n","\n","setid_mat_path = os.path.join(data_dir, 'setid.mat')\n","setid_mat = scipy.io.loadmat(setid_mat_path)\n","train_ids = setid_mat['trnid'][0] - 1\n","val_ids = setid_mat['valid'][0] - 1\n","test_ids = setid_mat['tstid'][0] - 1\n","\n","\n","image_dir = extracted_dir\n","image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n","\n","\n","df = pd.DataFrame({\n","    'filename': image_files,\n","    'label': labels\n","})\n","\n","# 학습, 검증, 테스트 세트 분할\n","train_df = df.iloc[train_ids].reset_index(drop=True)\n","val_df = df.iloc[val_ids].reset_index(drop=True)\n","test_df = df.iloc[test_ids].reset_index(drop=True)\n","\n","print(f\"학습 세트 크기: {len(train_df)}\")\n","print(f\"검증 세트 크기: {len(val_df)}\")\n","print(f\"테스트 세트 크기: {len(test_df)}\")\n","\n","\n","\n","\n","# 폴더\n","train_dir = os.path.join(data_dir, 'train')\n","val_dir = os.path.join(data_dir, 'val')\n","test_dir = os.path.join(data_dir, 'test')\n","\n","for directory in [train_dir, val_dir, test_dir]:\n","    os.makedirs(directory, exist_ok=True)\n","\n","    for label in df['label'].unique():\n","        os.makedirs(os.path.join(directory, str(label)), exist_ok=True)\n","\n","\n","def copy_images(df, target_dir):\n","    for _, row in df.iterrows():\n","        src = os.path.join(image_dir, row['filename'])\n","        dst = os.path.join(target_dir, str(row['label']), row['filename'])\n","        copyfile(src, dst)\n","\n","# 이미지 복사\n","print(\"학습 이미지 복사 중...\")\n","copy_images(train_df, train_dir)\n","print(\"검증 이미지 복사 중...\")\n","copy_images(val_df, val_dir)\n","print(\"테스트 이미지 복사 중...\")\n","copy_images(test_df, test_dir)\n","print(\"모든 이미지 복사 완료.\")\n","\n","\n","\n","!pip install timm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","\n","# 하이퍼파라미터\n","learning_rate = 1e-4\n","num_epochs = 10\n","num_classes = 102\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","from timm import create_model\n","\n","# 모델 불러오기\n","model = create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=num_classes)\n","model = model.to(device)\n","\n","# 손실함수, 옵티마이져 정의\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","train_dir = os.path.join(data_dir, 'train')\n","val_dir = os.path.join(data_dir, 'val')\n","test_dir = os.path.join(data_dir, 'test')\n","\n","# 데이터 변환\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]),\n","}\n","\n","# 데이터셋 정의\n","datasets = {\n","    'train': datasets.ImageFolder(train_dir, transform=data_transforms['train']),\n","    'val': datasets.ImageFolder(val_dir, transform=data_transforms['val']),\n","    'test': datasets.ImageFolder(test_dir, transform=data_transforms['test']),\n","}\n","\n","# DataLoader 생성\n","dataloaders = {\n","    x: DataLoader(datasets[x], batch_size=32, shuffle=(x == 'train'), num_workers=2)\n","    for x in ['train', 'val', 'test']\n","}\n","\n","# train\n","def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","        print(\"-\" * 20)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            for inputs, labels in dataloaders[phase]:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                #순전파\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    #역전파\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n","\n","    print(\"Training complete.\")\n","    return model\n","\n","model = train_model(model, dataloaders, criterion, optimizer, num_epochs)\n","\n","# 모델 평가\n","def evaluate_model(model, dataloader):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='macro')\n","    recall = recall_score(all_labels, all_preds, average='macro')\n","\n","    return accuracy, precision, recall\n","\n","\n","accuracy, precision, recall = evaluate_model(model, dataloaders['test'])\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","print(f\"Test Precision: {precision:.4f}\")\n","print(f\"Test Recall: {recall:.4f}\")\n","\n","\n","pip install torchprofile\n","from torchprofile import profile_macs\n","dummy_input = torch.randn(1, 3, 224, 224).to(device)\n","flops = profile_macs(model, dummy_input)\n","print(f\"FLOPs: {flops / 1e9:.4f} GFLOPs\")"]}]}